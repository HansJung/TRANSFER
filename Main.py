import numpy as np
import matplotlib.pyplot as plt

from DataGen import DataGen
from StoExp import StoExp
from JZ_bound import JZ_bound
from DUCB import DUCB

# Parameter configuration
D = 1 # Dimension of Z
N = 5000 # Number of samples
T = int(N/2) # Round of bandit simulation

seed_num = 7308595 # Case 2
# seed_num = 3478042 # Case 3
# seed_num = np.random.randint(10000000)

''' 
Configuration for Case setting 
> Case 3
### seed_num = 3478042 // D=1, N=5000, T=N/2

> Case 2
### seed_num = 7308595 // D=1, N=5000, T=N/2
'''

''' Step 0. Generating Observation data'''
datagen = DataGen(D,N,seed_num)
OBS = datagen.obs_data() # observational data

''' Step 1. Constructing policies '''
stoexp = StoExp(D)
[X,Y,Z] = stoexp.sepOBS(OBS)
## Policy 1. pi(x|z) is a logistic regression
### for making distinct results, pi(1-x|z) is trained.
obslogit = stoexp.Logit(1-X,Z)

## Policy 2. pi(x|z) is a XGboost
obsxgb = stoexp.XGB(X,Z)
policy_list = [obslogit,obsxgb]

## Policy data
poly_obslogit_data = datagen.poly_intv_data(obslogit, Z) # (X1,Y1,Z) from pi(x|z), pi:logit
poly_obsxgb_data = datagen.poly_intv_data(obsxgb, Z) # (X2,Y2,Z) from pi(x|z), pi:xgboost
X_pl_list = [poly_obslogit_data['X'], poly_obsxgb_data['X']] # list of (X1,X2)
Y_pl_list = [poly_obslogit_data['Y'], poly_obsxgb_data['Y']] # list of (Y1,Y2)

obslogit_pred = obslogit.predict_proba(Z) # list of [pi(X=0|zi), pi(X=1|zi)], i=1,2,...,N // pi:logit
obsxgb_pred = obsxgb.predict_proba(Z) # list of [pi(X=0|zi), pi(X=1|zi)], i=1,2,...,N // pi:xgboost
pred_list = [obslogit_pred, obsxgb_pred]

### True mean
Y_obslogit = np.mean(poly_obslogit_data['Y']) # mean(Y1),a reward of policy 1
Y_obsxgb = np.mean(poly_obsxgb_data['Y']) # mean(Y2),a reward of policy 2
Y_pis = [Y_obslogit, Y_obsxgb]

opt_pl = np.argmax([Y_obslogit, Y_obsxgb]) # optimal policy (index)
subopt_pl = 1-opt_pl # sub-optimal policy // Binary

opt_Ypi = Y_pis[opt_pl] #Y*, Y generated by selecting optimal policy
subopt_Ypi = Y_pis[subopt_pl] # Y-, Y generated by selecting sub-optimal policy

''' Step 3. Construct causal bound (JZ bound)'''
JZ = JZ_bound()
[L_obslogit, H_obslogit] = JZ.JZ_bounds(obslogit,OBS,D,N) # Lower, Upper of policy 1
[L_obsxgb, H_obsxgb] = JZ.JZ_bounds(obsxgb,OBS,D,N) # Lower, Upper of policy 2
Bdd = [[L_obslogit, H_obslogit],[L_obsxgb, H_obsxgb]]

h_pl = Bdd[subopt_pl][1] # h_pl, where pl is a policy idx
if h_pl < opt_Ypi:
    case_num = 2 # Case 2 in Bandit analysis
else:
    case_num = 3 # Case 3 in Bandit analysis

''' Step 4. Running DUCB '''
ducb = DUCB(policy_list,pred_list,opt_pl,T, X_pl_list,Y_pl_list,Z)
# Vanila D-UCB (Rajet sen, 2018)
prob_opt_list,avg_loss_list,num_pull = ducb.conduct_DUCB()
# Bounded D-UCB (B-DUCB) (JZ)
bdd_prob_opt_list,bdd_avg_loss_list,bdd_num_pull = ducb.conduct_BDUCB(Bdd)
print(case_num) # What case this bandit is in?

''' Step 5. Compring the result '''
## Probability of selecting optimal policy
plt.figure()
plt.title("Prob optimal policy")
plt.plot(prob_opt_list,label="DUCB")
plt.plot(bdd_prob_opt_list,label="B-DUCB")
plt.legend()

## Probability of average loss
plt.figure()
plt.title("Average loss")
plt.plot(avg_loss_list,label="DUCB")
plt.plot(bdd_avg_loss_list,label="B-DUCB")
plt.legend()


